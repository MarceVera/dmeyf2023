{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install tsfresh\n",
    "!pip install gcsfs openpyxl\n",
    "!pip install hdbscan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.utils import resample\n",
    "#from tsfresh import extract_features\n",
    "#from tsfresh.utilities.dataframe_functions import impute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "reducing.py\n",
    "Author: Kirgsn, 2018\n",
    "https://www.kaggle.com/code/etremblay/fail-safe-parallel-memory-reduction/notebook?scriptVersionId=27504964\n",
    "https://wkirgsn.github.io/2018/02/10/auto-downsizing-dtypes/\n",
    "[alternative] https://www.kaggle.com/gemartin/load-data-reduce-memory-usage\n",
    "              (tener en cuenta) https://www.kaggle.com/c/champs-scalar-coupling/discussion/96655\n",
    "\n",
    "Use like this:\n",
    ">>> from reducing import Reducer\n",
    ">>> df = Reducer().reduce(df)\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import gc\n",
    "# from joblib import Parallel, delayed\n",
    "# from tqdm import tqdm\n",
    "\n",
    "__all__ = [\"Reducer\"]\n",
    "\n",
    "\n",
    "class Reducer:\n",
    "    \"\"\"\n",
    "    Class that takes a dict of increasingly big numpy datatypes to transform\n",
    "    the data of a pandas dataframe into, in order to save memory usage.\n",
    "    \"\"\"\n",
    "\n",
    "    memory_scale_factor = 1024**2  # memory in MB\n",
    "\n",
    "    def __init__(self, conv_table=None, use_categoricals=True, n_jobs=-1):\n",
    "        \"\"\"\n",
    "        :param conv_table: dict with np.dtypes-strings as keys\n",
    "        :param use_categoricals: Whether the new pandas dtype \"Categoricals\"\n",
    "                shall be used\n",
    "        :param n_jobs: Parallelization rate\n",
    "        \"\"\"\n",
    "\n",
    "        self.conversion_table = conv_table or {\n",
    "            \"int\": [np.int8, np.int16, np.int32, np.int64],\n",
    "            \"uint\": [np.uint8, np.uint16, np.uint32, np.uint64],\n",
    "            \"float\": [np.float32],\n",
    "            \"datetime\": []\n",
    "        }\n",
    "        self.null_int = [\n",
    "            pd.Int8Dtype,\n",
    "            pd.Int16Dtype,\n",
    "            pd.Int32Dtype,\n",
    "            pd.Int64Dtype,\n",
    "            pd.UInt8Dtype,\n",
    "            pd.UInt16Dtype,\n",
    "            pd.UInt32Dtype,\n",
    "            pd.UInt64Dtype,\n",
    "        ]\n",
    "\n",
    "        self.use_categoricals = use_categoricals\n",
    "        self.n_jobs = n_jobs\n",
    "\n",
    "    def _type_candidates(self, k):\n",
    "        for c in self.conversion_table[k]:\n",
    "            i = np.iinfo(c) if \"int\" in k else np.finfo(c)\n",
    "            yield c, i\n",
    "\n",
    "\n",
    "    def reduce(self, df, verbose=False):\n",
    "        \"\"\"Takes a dataframe and returns it with all data transformed to the\n",
    "        smallest necessary types.\n",
    "\n",
    "        :param df: pandas dataframe\n",
    "        :param verbose: If True, outputs more information\n",
    "        :return: pandas dataframe with reduced data types\n",
    "        \"\"\"\n",
    "        mem_usage_orig = df.memory_usage().sum() / self.memory_scale_factor\n",
    "        start_time = time.time()\n",
    "\n",
    "        gc.enable()\n",
    "        df_reduced = df.apply(self._reduce, axis=0, args=(verbose,), n_jobs=self.n_jobs)\n",
    "\n",
    "        del df\n",
    "        gc.collect()\n",
    "\n",
    "        mem_usage_new = df_reduced.memory_usage().sum() / self.memory_scale_factor\n",
    "        end_time = time.time()\n",
    "        print(\n",
    "            f\"Reduced file from {mem_usage_orig:.2f} MB \"\n",
    "            f\"to {mem_usage_new:.2f} MB \"\n",
    "            f\"in {(end_time - start_time):.2f} seconds\"\n",
    "        )\n",
    "\n",
    "        return df_reduced\n",
    "\n",
    "    def _reduce(self, s, verbose, n_jobs):\n",
    "        try:\n",
    "            isnull = False\n",
    "            # skip NaNs\n",
    "            if s.isnull().any():\n",
    "                isnull = True\n",
    "            # detect kind of type\n",
    "            coltype = s.dtype\n",
    "            if np.issubdtype(coltype, np.integer):\n",
    "                conv_key = \"int\" if s.min() < 0 else \"uint\"\n",
    "            elif np.issubdtype(coltype, np.floating):\n",
    "                conv_key = \"float\"\n",
    "            else:\n",
    "                conv_key = coltype.name\n",
    "\n",
    "            # get smallest type\n",
    "            if conv_key in self.conversion_table:\n",
    "                candidates = self._type_candidates(conv_key)\n",
    "                for i, (t, info) in enumerate(candidates):\n",
    "                    if s.max() <= info.max and s.min() >= info.min:\n",
    "                        if verbose:\n",
    "                            print(f\"{s.name}: {coltype} -> {t}\")\n",
    "                        if isnull:\n",
    "                            s = s.astype(t, copy=False)\n",
    "                            s = s.where(s.notnull(), None)\n",
    "                        else:\n",
    "                            s = s.astype(t, copy=False)\n",
    "                        break\n",
    "            elif self.use_categoricals and \"category\" not in conv_key:\n",
    "                if verbose:\n",
    "                    print(f\"{s.name}: {coltype} -> category\")\n",
    "                s = s.astype(\"category\")\n",
    "            return s\n",
    "        except Exception:\n",
    "            print(f\"Error in column {s.name}: {sys.exc_info()[0]}\")\n",
    "            return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'gs://marcevera/datasets/competencia_02.csv.gz'\n",
    "df = pd.read_csv(file_path)\n",
    "df = Reducer().reduce(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "undersampled_data = pd.DataFrame()\n",
    "for month in df.foto_mes.unique():\n",
    "    \n",
    "    # Separar los bloques por clases\n",
    "    clase_baja_1 = df[(df['foto_mes'] == month) & (df['clase_ternaria'] == 'BAJA+1')]\n",
    "    clase_baja_2 = df[(df['foto_mes'] == month) & (df['clase_ternaria'] == 'BAJA+2')]\n",
    "    clase_continua = df[(df['foto_mes'] == month) & (df['clase_ternaria'] == 'CONTINUA')]\n",
    "\n",
    "    undersampled_continua = resample(clase_continua, replace=False, n_samples=max(10, int((len(clase_baja_1)+len(clase_baja_2))/2)), random_state=42)\n",
    "\n",
    "    # Concatenar los bloques undersampleados al DataFrame final\n",
    "    undersampled_data = pd.concat([undersampled_data, clase_baja_1, clase_baja_2, undersampled_continua])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir el tamaño del bloque para procesar en cada iteración\n",
    "block_size = 10000\n",
    "\n",
    "# Inicializar un DataFrame vacío para almacenar los resultados undersampleados\n",
    "undersampled_data = pd.DataFrame()\n",
    "\n",
    "# Leer el archivo CSV en bloques\n",
    "for chunk in pd.read_csv(file_path, chunksize=block_size):\n",
    "\n",
    "    # Separar los bloques por clases\n",
    "    clase_baja_1 = chunk[chunk['clase_ternaria'] == 'BAJA+1']\n",
    "    clase_baja_2 = chunk[chunk['clase_ternaria'] == 'BAJA+2']\n",
    "    clase_continua = chunk[chunk['clase_ternaria'] == 'CONTINUA']\n",
    "\n",
    "    undersampled_continua = resample(clase_continua, replace=False, n_samples=max(10, int((len(clase_baja_1)+len(clase_baja_2))/2)), random_state=42)\n",
    "\n",
    "    # Concatenar los bloques undersampleados al DataFrame final\n",
    "    undersampled_data = pd.concat([undersampled_data, clase_baja_1, clase_baja_2, undersampled_continua])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = undersampled_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diccionario_datos  = pd.read_excel('gs://marcevera/datasets/DiccionarioDatos_2023.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columnas_pesos = diccionario_datos[diccionario_datos['unidad'] == 'pesos']['campo'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear nuevas columnas divididas por el promedio para cada foto_mes\n",
    "for columna in df.columns:\n",
    "    # columna = columna.lower()\n",
    "    if columna in columnas_pesos:\n",
    "        promedio_por_mes = df.groupby('foto_mes')[columna].transform('mean')\n",
    "        nueva_columna = f'{columna}_div_promedio'\n",
    "        df[nueva_columna] = df[columna] / promedio_por_mes\n",
    "        df = df.drop(columna, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Convertir la columna 'foto_mes' a formato de fecha\n",
    "df['foto_mes'] = pd.to_datetime(df['foto_mes'], format='%Y%m')\n",
    "\n",
    "# 2. Asegurarte de tener una columna 'id'\n",
    "#df['id'] = df['numero_de_cliente']\n",
    "\n",
    "# 3. Identificar columnas numéricas para aplicar interpolación\n",
    "columnas_numericas = df.select_dtypes(include='number').columns\n",
    "\n",
    "# 4. Aplicar interpolación solo a columnas numéricas\n",
    "df[columnas_numericas] = df[columnas_numericas].interpolate(method='linear', axis=0)\n",
    "df = df.ffill().bfill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_numeric = df[columnas_numericas]\n",
    "df_numeric['foto_mes'] = df['foto_mes']\n",
    "\n",
    "# 6. Extraer características temporales con TSFRESH\n",
    "extracted_features = extract_features(df_numeric, column_id='id', column_sort='foto_mes', n_jobs=16)\n",
    "\n",
    "# 7. Agregar la columna 'clase_ternaria' a las características extraídas\n",
    "extracted_features['clase_ternaria'] = df.groupby('id')['clase_ternaria'].first()\n",
    "\n",
    "# 8. Imputar valores faltantes\n",
    "extracted_features = impute(extracted_features)\n",
    "\n",
    "# Ahora, `extracted_features` contiene las características temporales generadas por TSFRESH con imputación de valores NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Convertir la columna 'clase_ternaria' a valores numéricos (por ejemplo, 0 para CONTINUA, 1 para BAJA+1, 2 para BAJA+2)\n",
    "df['clase_ternaria'] = df['clase_ternaria'].map({'CONTINUA': 0, 'BAJA+1': 1, 'BAJA+2': 2})\n",
    "\n",
    "# Dividir el dataset en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.drop(['clase_ternaria', 'foto_mes'], axis=1), df['clase_ternaria'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Entrenar un modelo de clasificación (por ejemplo, Random Forest)\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluar el modelo en el conjunto de prueba\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# Visualizar la importancia de las características\n",
    "feature_importances = clf.feature_importances_\n",
    "feature_names = X_train.columns\n",
    "feature_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importances})\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Visualizar las 15 características más importantes\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x='Importance', y='Feature', data=feature_importance_df.head(15), palette='viridis')\n",
    "plt.title('Top 15 Características Importantes')\n",
    "plt.show()\n",
    "\n",
    "# Seleccionar características importantes\n",
    "sfm = SelectFromModel(clf, threshold=0.01)\n",
    "sfm.fit(X_train, y_train)\n",
    "selected_features = X_train.columns[sfm.get_support()]\n",
    "\n",
    "# Imprimir las características seleccionadas\n",
    "print(\"\\nCaracterísticas seleccionadas:\")\n",
    "print(selected_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import hdbscan\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "# Cargar el dataset\n",
    "#file_path = 'ruta/del/tu/dataset.csv'  # Reemplaza con la ruta real de tu archivo CSV\n",
    "#df = pd.read_csv(file_path)\n",
    "\n",
    "# Reemplazar NaN con valores adecuados (puedes usar la media o la mediana)\n",
    "#df.fillna(df.mean(), inplace=True)\n",
    "\n",
    "df_filtered = df[df['clase_ternaria'].isin(['BAJA+2', 'CONTINUA'])]\n",
    "\n",
    "var_thresh = VarianceThreshold(threshold=0.1)\n",
    "transformed_df = var_thresh.fit_transform(df_filtered.select_dtypes('number'))\n",
    "# Filtrar filas relevantes solo para 'BAJA+2' y 'CONTINUA'\n",
    "#df_filtered = transformed_df[transformed_df['clase_ternaria'].isin(['BAJA+2', 'CONTINUA'])]\n",
    "\n",
    "# Seleccionar características relevantes (puedes personalizar esta parte según tus necesidades)\n",
    "features = transformed_df\n",
    "#.drop(['clase_ternaria', 'foto_mes'], axis=1)\n",
    "\n",
    "# Estandarizar las características\n",
    "scaler = StandardScaler()\n",
    "features_scaled = scaler.fit_transform(features)\n",
    "\n",
    "# Reducción de dimensionalidad con PCA (puedes ajustar el número de componentes según tus necesidades)\n",
    "pca = PCA(n_components=2)\n",
    "features_pca = pca.fit_transform(features_scaled)\n",
    "\n",
    "# Aplicar HDBSCAN\n",
    "clusterer = hdbscan.HDBSCAN(min_cluster_size=50, min_samples=5)\n",
    "clusters = clusterer.fit_predict(features_pca)\n",
    "\n",
    "# Visualizar los resultados\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.scatterplot(x=features_pca[:, 0], y=features_pca[:, 1], hue=clusters, palette='viridis', legend='full')\n",
    "plt.title('HDBSCAN Clustering Result')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.legend(title='Cluster')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import hdbscan\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Cargar el dataset\n",
    "#file_path = 'ruta/del/tu/dataset.csv'  # Reemplaza con la ruta real de tu archivo CSV\n",
    "#df = pd.read_csv(file_path)\n",
    "\n",
    "# Reemplazar NaN con valores adecuados (puedes usar la media o la mediana)\n",
    "#df.fillna(df.mean(), inplace=True)\n",
    "\n",
    "# Convertir la columna 'clase_ternaria' a valores numéricos (por ejemplo, 0 para CONTINUA, 1 para BAJA+1, 2 para BAJA+2)\n",
    "df['clase_ternaria'] = df['clase_ternaria'].map({'CONTINUA': 0, 'BAJA+1': 1, 'BAJA+2': 2})\n",
    "\n",
    "# Estandarizar las características\n",
    "scaler = StandardScaler()\n",
    "features_scaled = scaler.fit_transform(df.drop(['foto_mes', 'clase_ternaria'], axis=1))\n",
    "\n",
    "# Reducción de dimensionalidad con PCA (puedes ajustar el número de componentes según tus necesidades)\n",
    "pca = PCA(n_components=2)\n",
    "features_pca = pca.fit_transform(features_scaled)\n",
    "\n",
    "# Crear un DataFrame para almacenar los resultados de cada mes\n",
    "results_by_month = pd.DataFrame(columns=['foto_mes', 'cluster'])\n",
    "\n",
    "# Iterar sobre los meses\n",
    "for month in df['foto_mes'].unique():\n",
    "    # Filtrar el DataFrame por el mes actual\n",
    "    df_month = df[df['foto_mes'] == month]\n",
    "\n",
    "    # Seleccionar características relevantes (puedes personalizar esta parte según tus necesidades)\n",
    "    features_month = df_month.drop(['foto_mes', 'clase_ternaria'], axis=1)\n",
    "\n",
    "    # Estandarizar las características\n",
    "    features_scaled_month = scaler.transform(features_month)\n",
    "\n",
    "    # Aplicar HDBSCAN\n",
    "    clusterer = hdbscan.HDBSCAN(min_cluster_size=50, min_samples=5)\n",
    "    clusters = clusterer.fit_predict(features_scaled_month)\n",
    "\n",
    "    # Almacenar los resultados en el DataFrame\n",
    "    results_by_month = pd.concat([results_by_month, pd.DataFrame({'foto_mes': [month] * len(clusters), 'cluster': clusters})])\n",
    "\n",
    "# Visualizar los resultados globales\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.scatterplot(x=features_pca[:, 0], y=features_pca[:, 1], hue=results_by_month['cluster'], palette='viridis', legend='full')\n",
    "plt.title('HDBSCAN Clustering Result - Global')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.legend(title='Cluster')\n",
    "plt.show()\n",
    "\n",
    "# Visualizar los resultados por mes\n",
    "plt.figure(figsize=(16, 8))\n",
    "sns.scatterplot(x=features_pca[:, 0], y=features_pca[:, 1], hue=results_by_month['foto_mes'], palette='viridis', legend='full')\n",
    "plt.title('HDBSCAN Clustering Result - Por Mes')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.legend(title='Mes')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import hdbscan\n",
    "import umap\n",
    "import os\n",
    "\n",
    "# Cargar el dataset\n",
    "#file_path = 'ruta/del/tu/dataset.csv'  # Reemplaza con la ruta real de tu archivo CSV\n",
    "#df = pd.read_csv(file_path)\n",
    "\n",
    "# Reemplazar NaN con valores adecuados (puedes usar la media o la mediana)\n",
    "#df.fillna(df.mean(), inplace=True)\n",
    "\n",
    "# Convertir la columna 'clase_ternaria' a valores numéricos (por ejemplo, 0 para CONTINUA, 1 para BAJA+1, 2 para BAJA+2)\n",
    "#df['clase_ternaria'] = df['clase_ternaria'].map({'CONTINUA': 0, 'BAJA+1': 1, 'BAJA+2': 2})\n",
    "\n",
    "# Crear un directorio para almacenar los gráficos\n",
    "output_directory = 'graficos_por_mes'\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# Iterar sobre los meses\n",
    "for month in df['foto_mes'].unique():\n",
    "    # Filtrar el DataFrame por el mes actual\n",
    "    df_month = df[df['foto_mes'] == month]\n",
    "\n",
    "    # Seleccionar características relevantes (puedes personalizar esta parte según tus necesidades)\n",
    "    features_month = df_month.drop(['foto_mes', 'clase_ternaria'], axis=1)\n",
    "\n",
    "    # Estandarizar las características\n",
    "    scaler = StandardScaler()\n",
    "    features_scaled_month = scaler.fit_transform(features_month)\n",
    "\n",
    "    # Aplicar UMAP para reducción de dimensionalidad\n",
    "    reducer = umap.UMAP(n_components=2)\n",
    "    features_umap = reducer.fit_transform(features_scaled_month)\n",
    "\n",
    "    # Aplicar HDBSCAN\n",
    "    clusterer = hdbscan.HDBSCAN(min_cluster_size=50, min_samples=5)\n",
    "    clusters = clusterer.fit_predict(features_scaled_month)\n",
    "\n",
    "    # Almacenar el resultado del clustering en el DataFrame original\n",
    "    df_month['cluster'] = clusters\n",
    "\n",
    "    # Visualizar los resultados por mes\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.scatterplot(x=features_umap[:, 0], y=features_umap[:, 1], hue=clusters, palette='viridis', legend='full')\n",
    "    plt.title(f'HDBSCAN Clustering Result - Mes {month}')\n",
    "    plt.xlabel('UMAP Component 1')\n",
    "    plt.ylabel('UMAP Component 2')\n",
    "    plt.legend(title='Cluster')\n",
    "    \n",
    "    # Guardar el gráfico en el directorio\n",
    "    plt.savefig(os.path.join(output_directory, f'clustering_result_mes_{month}.png'))\n",
    "    plt.close()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
